tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))
tfidf_matrix = tfidf.fit_transform(jokes['Text'].fillna(''))
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
def get_recommendations(joke_id, sim_matrix, top_n=5):
    if joke_id < 1 or joke_id > 150:
        print("Invalid Joke Id")
        return

    idx = jokes.index.get_loc(joke_id)
    sim_scores = list(enumerate(sim_matrix[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1],
                        reverse=True)[1:top_n + 1]

    indices = [i[0] for i in sim_scores]
    return jokes.iloc[indices][['Text']]
recc = get_recommendations(40, cosine_sim)
recc

def experiment_lsa_components(tfidf_matrix, components_range):
    explained_variances = []

    for n in components_range:
        svd = TruncatedSVD(n_components=n, random_state=42)
        svd.fit(tfidf_matrix)
        total_variance = svd.explained_variance_ratio_.sum()
        explained_variances.append(total_variance)
        print(f"Components: {n} | Explained Variance: {total_variance:.4f}")

    return explained_variances

n_comp_list = np.arange(2,145,step=5)
variances = experiment_lsa_components(tfidf_matrix, n_comp_list)

plt.figure(figsize=(10, 6))
plt.plot(n_comp_list, variances, 'bo-', linewidth=2)
plt.axhline(y=0.9, color='r', linestyle='--', label='90% Variance Threshold')

plt.title('Selecting Optimal Components for LSA')
plt.xlabel('Number of Components (Latent Topics)')
plt.ylabel('Cumulative Explained Variance')
plt.grid(True, alpha=0.3)
plt.legend()
plt.show()

svd_final = TruncatedSVD(n_components=110, random_state=42)
lsa_matrix_final = svd_final.fit_transform(tfidf_matrix)
cosine_sim_lsa = cosine_similarity(lsa_matrix_final)

test_id = 10 
print(jokes.iloc[test_id,0])
print("--- Text Matching (TF-IDF - only words) ---")
print(get_recommendations(test_id, cosine_sim)) 

print("\n--- Semantic similarity (LSA 110 components) ---")
print(get_recommendations(test_id, cosine_sim_lsa))

ratings = pd.read_csv('../Data/processed_ratings.csv',
                      delimiter=',',
                      header=None)
ratings_matrix = ratings.iloc[:,1:]
ratings_raw = ratings_matrix.replace(99, np.nan).values
mask = ~np.isnan(ratings_raw)

def create_svd_pipeline(n_components):
    return Pipeline([
        ('imputer', SimpleImputer(strategy='mean')),
        ('svd', TruncatedSVD(n_components=n_components, random_state=42))
    ])

actual_values = ratings_raw[mask]
components_list = [2, 5, 10, 20, 30, 40, 50, 75, 100, 105, 110]
rmse_results = []

for n in components_list:
    pipe = create_svd_pipeline(n)

    user_features = pipe.fit_transform(ratings_raw)
    vt = pipe.named_steps['svd'].components_
    
    predicted_matrix = np.dot(user_features, vt)
    
    rmse = np.sqrt(mean_squared_error(actual_values, predicted_matrix[mask]))
    rmse_results.append(rmse)
    print(f"Components: {n} | RMSE: {rmse:.4f}")


plt.figure(figsize=(10, 6))
plt.plot(components_list, rmse_results, 'ro-', linewidth=2)
plt.title('RMSE vs Number of Latent Factors (Collaborative Filtering)')
plt.xlabel('Number of Latent Factors')
plt.ylabel('RMSE')
plt.grid(True, alpha=0.3)
plt.show()